{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import pi\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.quantum_info import Statevector\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit_aer.primitives import SamplerV2 as Sampler\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "# from qiskit_aer.noise import NoiseModel\n",
    "# from qiskit_ibm_provider import IBMProvider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = 100\n",
    "epsilon_loss = 1e-3 # loss added if test_counts entry is not found\n",
    "epsilon_parameters=0.1 # scaling factor for initial random parameters (chosen between 0 to 2pi otherwise)\n",
    "\n",
    "n_qubits = 3\n",
    "n_bases = 27\n",
    "depth = 10\n",
    "\n",
    "max_iter = 1000\n",
    "\n",
    "param_name = \"theta\"\n",
    "\n",
    "# save checkpoints in case the connection to qpu breaks\n",
    "# useful for running on real qpu, but not necessary for local simulation\n",
    "have_checkpoints = False\n",
    "checkpoint_path = \"Alt_ideal_ghz_q3_b27_d6_sh100_checkpoints\"\n",
    "\n",
    "#seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(seed)\n",
    "#rng = np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if have_checkpoints:\n",
    "#     os.mkdir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend = AerSimulator()\n",
    "# sampler = Sampler.from_backend(backend)\n",
    "# pm = generate_preset_pass_manager(backend=backend)\n",
    "\n",
    "simulator_ideal = AerSimulator()\n",
    "#simulator_ideal.set_options(seed_simulator=seed)\n",
    "# sampler_ideal = Sampler.from_backend(simulator_ideal)\n",
    "pm_ideal = generate_preset_pass_manager(backend=simulator_ideal, optimization_level=1)\n",
    "\n",
    "# sampler = sampler_ideal\n",
    "pm = pm_ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ghz_state(n):\n",
    "    qc = QuantumCircuit(n)\n",
    "    qc.h(0)\n",
    "    for i in range(n - 1):\n",
    "        qc.cx(i, i + 1)\n",
    "    return qc\n",
    "\n",
    "def get_circ_for_basis(qc: QuantumCircuit, basis: str):\n",
    "    circ = qc.copy()\n",
    "    for i, s in enumerate(basis[::-1]): # here, I am using the same endian convention as qiskit, where the last bit represent the first qubit\n",
    "        if s == 'Z':\n",
    "            pass\n",
    "        elif s == 'X':\n",
    "            circ.ry(-pi/2, i)\n",
    "        elif s == 'Y':\n",
    "            circ.rx(pi/2, i)\n",
    "        else:\n",
    "            raise ValueError(f\"{s} is invalid basis\")\n",
    "\n",
    "    circ.measure_all()\n",
    "\n",
    "    return circ\n",
    "\n",
    "def get_isa_circ_list_for_bases(qc: QuantumCircuit, basis_list, pass_manager):\n",
    "    circ_list = [get_circ_for_basis(qc, basis) for basis in basis_list]\n",
    "    isa_circ_list = pass_manager.run(circ_list)\n",
    "    return isa_circ_list\n",
    "\n",
    "\n",
    "def measure_isa_circ_list_fs(isa_circ_list, basis_list, shots=shots):\n",
    "    #sampler = Sampler(seed =rng.integers(1000000) )\n",
    "    sampler = Sampler()\n",
    "    result_list = sampler.run(isa_circ_list, shots=shots).result()\n",
    "    counts_list = [result.data.meas.get_counts() for result in result_list]\n",
    "    results_dic = {basis:counts for (basis, counts) in zip(basis_list, counts_list)}\n",
    "    return results_dic\n",
    "\n",
    "# randomly chooses n_bases from possible basis (no duplicates)\n",
    "def get_basis_list(n_bases,n_qubits):\n",
    "    bases = [\"X\", \"Y\", \"Z\"]\n",
    "    basis_list = [\"\".join(p) for p in itertools.product(bases, repeat=n_qubits)]\n",
    "    culled_basis_list = random.sample(basis_list, n_bases)\n",
    "    return culled_basis_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def measure_circ_for_all_basis(qc, basis_list, shots=total_shots, sampler=None):\n",
    "#     results_dic = {}\n",
    "#     qc_list = [get_circ_for_basis(qc, basis, simulator) for basis in basis_list]\n",
    "#     job = simulator.run(qc_list, shots=shots)\n",
    "#     result = job.result()\n",
    "#     counts_list = result.get_counts()\n",
    "#     results_dic = {basis:counts for (basis, counts) in zip(basis_list, counts_list)}\n",
    "    \n",
    "#     return results_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_list = get_basis_list(n_bases, n_qubits)\n",
    "ghz_circ = create_ghz_state(n_qubits)\n",
    "ghz_circ_isa_list = get_isa_circ_list_for_bases(ghz_circ, basis_list, pm_ideal)\n",
    "GHZ_measurement = measure_isa_circ_list_fs(ghz_circ_isa_list, basis_list, shots=shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(basis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GHZ_measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_theta_random(circ_depth=10, num_qbits=3):\n",
    "    \"\"\"\n",
    "    Initialize the theta parameter vector\n",
    "    :param circ_depth: int, number of parameterized layers in circuit\n",
    "    :param num_qbits: int, number of qbits\n",
    "    :return: np.array, values of theta\n",
    "    \"\"\"\n",
    "\n",
    "    #theta = rng.random(size=(circ_depth, num_qbits))\n",
    "    theta = np.random.rand(circ_depth, num_qbits)\n",
    "    theta = 2*pi*epsilon_parameters*theta\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initialize_theta_random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_variational_circ_ansatz(num_qbits, circ_depth, param_name = param_name):\n",
    "    \"\"\"\n",
    "    Generate a parameterized variational quantum circuit\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    num_parameters = num_qbits * circ_depth\n",
    "    theta_vec = ParameterVector(param_name, length=num_parameters)\n",
    "    var_circ = QuantumCircuit(num_qbits)\n",
    "\n",
    "\n",
    "    for layer in range(circ_depth - 1):\n",
    "        # Compute if layer is odd or even to apply rx or ry gate to circuit\n",
    "        is_odd_step = (layer + 1) % 2\n",
    "\n",
    "        for qbit in range(num_qbits):\n",
    "            if is_odd_step:\n",
    "                var_circ.rx(theta_vec[layer * num_qbits + qbit], qbit)\n",
    "            else:\n",
    "                var_circ.ry(theta_vec[layer * num_qbits + qbit], qbit)\n",
    "\n",
    "        # Apply CX gates\n",
    "        for qbit in range((1-is_odd_step), num_qbits-1, 2):\n",
    "            # isOddStep may subtract 1 if True, to correctly apply cx gate location\n",
    "            var_circ.cx(qbit , qbit + 1)\n",
    "            var_circ.barrier() # for visualization only\n",
    "\n",
    "    for qbit in range(num_qbits):  # bonus layer at the end only has rx gates and no cx\n",
    "        var_circ.rx(theta_vec[(circ_depth - 1) * num_qbits + qbit], qbit)\n",
    "\n",
    "    return var_circ, theta_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz, param_vec = construct_variational_circ_ansatz(n_qubits, depth, param_name=param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz.draw(output=\"mpl\", style=\"clifford\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz_isa_list = get_isa_circ_list_for_bases(ansatz, basis_list, pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ansatz_output(ansatz_isa_list, parameters, basis_list, shots, param_placeholder=param_vec):\n",
    "    circ_run_list = [circ.assign_parameters({param_placeholder:parameters}) for circ in ansatz_isa_list]\n",
    "    return measure_isa_circ_list_fs(isa_circ_list=circ_run_list, basis_list=basis_list, shots=shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(true_data, test_data, epsilon_loss = epsilon_loss):\n",
    "    loss = 0\n",
    "\n",
    "    # here, let me assume true data and test data can have different number of shots\n",
    "    # but the shot in each basis should be the same\n",
    "    true_data_total_shots = sum(list(true_data.values())[0].values())\n",
    "    test_data_total_shots = sum(list(test_data.values())[0].values())\n",
    "\n",
    "    for basis in true_data: # just a note that if there are hallucinated measurements in \n",
    "        # bases/states not included in the true data then the kl divergence will not account for these (at least as presented in the paper)\n",
    "        for state in true_data[basis]:\n",
    "            true_counts = true_data[basis][state]\n",
    "            test_counts = test_data[basis].get(state, 0)\n",
    "            true_prob   = true_counts / true_data_total_shots\n",
    "            test_prob   = test_counts / test_data_total_shots\n",
    "            loss += true_prob *(np.log((true_prob)/(test_prob +epsilon_loss)))\n",
    "\n",
    "    return loss / len(true_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global loss_values\n",
    "global thetas\n",
    "\n",
    "loss_values = []\n",
    "thetas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_loss(theta_vector, ansatz_isa_list,  basis_list, true_data, shots, param_placeholder=param_vec, \\\n",
    "        print_message=True, have_checkpoints=have_checkpoints, checkpoint_path=checkpoint_path):\n",
    "    # theta = np.reshape(theta_vector, (circ_depth, num_qbits))\n",
    "    # tempcirc = construct_variational_circ(theta=theta)\n",
    "    test_data = get_ansatz_output(ansatz_isa_list=ansatz_isa_list, parameters=theta_vector, basis_list=basis_list, \\\n",
    "        shots=shots, param_placeholder=param_placeholder)\n",
    "    # test_data = measure_circ_for_all_basis(qc = tempcirc, basis_list=basis_list, simulator=simulator)\n",
    "    loss = KL_divergence(true_data, test_data) + \\\n",
    "        KL_divergence(test_data, true_data) # symmetrizing the loss\n",
    "\n",
    "    global loss_values\n",
    "    global thetas \n",
    "    loss_values.append(loss)\n",
    "    thetas.append(theta_vector)\n",
    "\n",
    "    num_iter = len(loss_values)\n",
    "\n",
    "    if print_message:\n",
    "        print(f\"Iteration {num_iter:5}; current loss is {loss}\")\n",
    "\n",
    "    if have_checkpoints:\n",
    "        # save checkpoints in case the connection to qpu breaks\n",
    "        # useful for running on real qpu, but not necessary for local simulation\n",
    "        np.save(os.path.join(checkpoint_path, f\"theta_iter{num_iter}.npy\"), thetas)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fidelity(psi, phi):\n",
    "    \"\"\"\n",
    "    Compute the fidelity (a measure of similarity) between the two states\n",
    "    :param psi: qiskit.Statevector, our target state |psi>\n",
    "    :param phi: qiskit.Statevector, our estimated state |phi>\n",
    "    :return: float, fidelity\n",
    "    \"\"\"\n",
    "\n",
    "    fidelity = qiskit.quantum_info.state_fidelity(psi, phi)\n",
    "    print(f\"psi: {psi}, phi: {phi}\")\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    # plot_bloch_multivector(psi)\n",
    "    # axes[0].set_title(\"psi\")\n",
    "    # plot_bloch_multivector(phi)\n",
    "    # axes[1].set_title(\"phi\")\n",
    "    return fidelity\n",
    "\n",
    "\n",
    "\n",
    "ghz_state = Statevector.from_instruction(ghz_circ)\n",
    "#ansatz_state = Statevector.from_instruction(ansatz.assign_parameters({param_vec : results.x}))\n",
    "\n",
    "#final_fidelity = qiskit.quantum_info.state_fidelity(ghz_state, ansatz_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_loss_filled(theta_vector):\n",
    "    return compute_kl_loss(theta_vector, ansatz_isa_list, basis_list, GHZ_measurement, shots, param_vec, print_message=True)\n",
    "\n",
    "def compute_kl_loss_filled_quiet(theta_vector):\n",
    "    return compute_kl_loss(theta_vector, ansatz_isa_list, basis_list, GHZ_measurement, shots, param_vec, print_message=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_compute_kl_loss(theta_vector, ansatz_isa_list,  basis_list, true_data, shots, param_placeholder=param_vec, \\\n",
    "        print_message=True, have_checkpoints=have_checkpoints, checkpoint_path=checkpoint_path):\n",
    "    # theta = np.reshape(theta_vector, (circ_depth, num_qbits))\n",
    "    # tempcirc = construct_variational_circ(theta=theta)\n",
    "    test_data = get_ansatz_output(ansatz_isa_list=ansatz_isa_list, parameters=theta_vector, basis_list=basis_list, \\\n",
    "        shots=shots, param_placeholder=param_placeholder)\n",
    "    # test_data = measure_circ_for_all_basis(qc = tempcirc, basis_list=basis_list, simulator=simulator)\n",
    "    loss = KL_divergence(true_data, test_data) + \\\n",
    "        KL_divergence(test_data, true_data) # symmetrizing the loss\n",
    "\n",
    "    global loss_values\n",
    "    global thetas \n",
    "    loss_values.append(loss)\n",
    "    thetas.append(theta_vector)\n",
    "\n",
    "    num_iter = len(loss_values)\n",
    "\n",
    "    if print_message:\n",
    "        print(f\"Iteration {num_iter:5}; current loss is {loss}\")\n",
    "\n",
    "    if have_checkpoints:\n",
    "        # save checkpoints in case the connection to qpu breaks\n",
    "        # useful for running on real qpu, but not necessary for local simulation\n",
    "        np.save(os.path.join(checkpoint_path, f\"theta_iter{num_iter}.npy\"), thetas)\n",
    "    \n",
    "    return (loss, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_algorithms.gradients import ParamShiftSamplerGradient\n",
    "from qiskit.primitives import Sampler as qiskitSampler\n",
    "\n",
    "def param_shift_gradient(theta_vector, ansatz_isa_list, basis_list, shots=shots):\n",
    "    #sampler = qiskitSampler(options={\"shots\": shots, \"seed\": rng.integers(1000000)})\n",
    "    sampler = qiskitSampler(options={\"shots\": shots})\n",
    "    param_shift_sampler_gradient = ParamShiftSamplerGradient(sampler)\n",
    "\n",
    "    prob_gradients = param_shift_sampler_gradient.run(ansatz_isa_list, [theta_vector for _ in ansatz_isa_list], None).result().gradients\n",
    "\n",
    "    results_dic = {basis:counts for (basis, counts) in zip(basis_list, prob_gradients)}\n",
    "    return results_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the gradient of the symmetrized kl loss on the theta parameters\n",
    "def KL_div_symm_gradient(gradients, true_data, test_data, num_parameters, epsilon_loss = epsilon_loss):\n",
    "    kl_gradients = np.zeros(num_parameters)\n",
    "\n",
    "    for parameter_index in range(num_parameters):\n",
    "        grad_true_test = 0\n",
    "        grad_test_true = 0\n",
    "\n",
    "        # here, let me assume true data and test data can have different number of shots\n",
    "        # but the shot in each basis should be the same\n",
    "        true_data_total_shots = sum(list(true_data.values())[0].values())\n",
    "        test_data_total_shots = sum(list(test_data.values())[0].values())\n",
    "\n",
    "        for basis in true_data: # just a note that if there are hallucinated measurements in \n",
    "            # bases/states not included in the true data then the kl divergence will not account for these (at least as presented in the paper)\n",
    "            for state in true_data[basis]:\n",
    "                true_counts = true_data[basis][state]\n",
    "                test_counts = test_data[basis].get(state, 0)\n",
    "                true_prob   = true_counts / true_data_total_shots\n",
    "                test_prob   = test_counts / test_data_total_shots\n",
    "\n",
    "                gradient_test = gradients[basis][parameter_index].get(int(state, 2), 0)\n",
    "                grad_true_test -= gradient_test * ((true_prob) / (test_prob +epsilon_loss))     \n",
    "                \n",
    "        for basis in test_data: \n",
    "            for state in test_data[basis]:\n",
    "                true_counts = true_data[basis].get(state, 0)\n",
    "                test_counts = test_data[basis][state]\n",
    "                true_prob   = true_counts / true_data_total_shots\n",
    "                test_prob   = test_counts / test_data_total_shots\n",
    "                \n",
    "                gradient_test = gradients[basis][parameter_index].get(int(state, 2), 0)          \n",
    "                grad_test_true += gradient_test * (1 + np.log((test_prob) / (true_prob +epsilon_loss)))\n",
    "\n",
    "        grad_true_test /= len(true_data)\n",
    "        grad_test_true /= len(test_data)\n",
    "\n",
    "        kl_gradients[parameter_index] = grad_true_test + grad_test_true\n",
    "\n",
    "    return kl_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_algorithms.optimizers import OptimizerResult\n",
    "\n",
    "# I also added an extra convergence checker for adam as well\n",
    "def param_shift_gradient_descent_adam(maxiter, learning_rate, decay_rate1, decay_rate2, epsilon, initial_theta, basis_list, true_data, shots, print_message=True, inst_stop_loss = 0.08, conv_threshold=0):\n",
    "    result = OptimizerResult()\n",
    "    result.nfev = 0\n",
    "    cur_theta = initial_theta\n",
    "    past_loss = 100000000000\n",
    "    norm_adjusted_gradient = 100000000000\n",
    "\n",
    "    E_g2 = np.zeros(initial_theta.shape)\n",
    "    E_g = np.zeros(initial_theta.shape)\n",
    "\n",
    "    for iter in range(maxiter):\n",
    "        (cur_loss, test_data) = mod_compute_kl_loss(cur_theta, ansatz_isa_list, basis_list, GHZ_measurement, shots, param_vec, print_message=False)\n",
    "        result.nfev += 1\n",
    "\n",
    "        if print_message:\n",
    "            print(f\"ITERATION: {iter+1}; loss at start of iteration is {cur_loss} \\t Relative change in loss {(cur_loss-past_loss)/past_loss}\")\n",
    "        \n",
    "        # When resullt is good enough we can exit early in order to save time\n",
    "        # if we found a very low loss we just break\n",
    "        if (cur_loss < inst_stop_loss):\n",
    "            print(f\"Found Small Loss EARLY at start of iter: {iter+1}\")\n",
    "            break\n",
    "        # handles convergence by checking if gradient was small enough\n",
    "        if (norm_adjusted_gradient < conv_threshold and (cur_loss-past_loss) < 0):\n",
    "            print(f\"Found Convergence EARLY at start of iter: {iter + 1}\")\n",
    "            break\n",
    "        \n",
    "        # calcualting kl gradient using parameter shift rule\n",
    "        circuit_gradients = param_shift_gradient(cur_theta, ansatz_isa_list, basis_list, shots=shots)\n",
    "        result.nfev += 2 * len(cur_theta)\n",
    "        kl_gradient = KL_div_symm_gradient(circuit_gradients, true_data, test_data, len(cur_theta))\n",
    "        \n",
    "        # applying adam propogation\n",
    "        E_g = decay_rate1 * E_g + (1-decay_rate1) * kl_gradient\n",
    "        E_g2 = decay_rate2 * E_g2 + (1-decay_rate2) * np.square(kl_gradient)\n",
    "\n",
    "        bias_cor_E_g = E_g / (1 - decay_rate1 ** (iter+1))\n",
    "        bias_cor_E_g2 = E_g2 / (1 - decay_rate2 ** (iter+1))\n",
    "        \n",
    "        adjusted_gradient = bias_cor_E_g / (np.sqrt(bias_cor_E_g2) + epsilon)\n",
    "        cur_theta = cur_theta - learning_rate * adjusted_gradient\n",
    "        \n",
    "        # saves the norm adjusted gradient  \n",
    "        norm_adjusted_gradient = np.linalg.norm(adjusted_gradient)\n",
    "        \n",
    "        if(print_message):\n",
    "            print(f\"Norm of Adam adjusted gradient: {norm_adjusted_gradient}\")\n",
    "            \n",
    "        past_loss = cur_loss\n",
    "\n",
    "\n",
    "\n",
    "    (loss, _) = mod_compute_kl_loss(cur_theta, ansatz_isa_list, basis_list, GHZ_measurement, shots, param_vec, print_message=print_message)\n",
    "    result.nfev += 1\n",
    "    \n",
    "    result.fun = loss\n",
    "    result.jac = kl_gradient\n",
    "    result.x = cur_theta\n",
    "    return result\n",
    "\n",
    "def finite_difference_forward_adam(loss_func, maxiter, learning_rate, decay_rate1, decay_rate2, epsilon, initial_theta, pertebation=0.1, print_message=True, inst_stop_loss = 0.09, conv_threshold=0.35):\n",
    "    cur_theta = initial_theta\n",
    "    past_loss = 100000000000\n",
    "    norm_adjusted_gradient = 100000000000\n",
    "\n",
    "    h = np.zeros(initial_theta.shape)\n",
    "    fd_gradient = np.zeros(initial_theta.shape)\n",
    "    E_g2 = np.zeros(initial_theta.shape)\n",
    "    E_g = np.zeros(initial_theta.shape)\n",
    "    \n",
    "    for iter in range(maxiter):\n",
    "        # calculating gradient according to finite difference method\n",
    "        cur_loss = loss_func(cur_theta)\n",
    "        if print_message:\n",
    "            print(f\"ITERATION: {iter+1}; loss at start of iteration is {cur_loss}  \\t Relative change in loss {(cur_loss-past_loss)/past_loss}\")\n",
    "        \n",
    "        # When resullt is good enough we can exit early in order to save time\n",
    "        # if we found a very low loss we just break\n",
    "        if (cur_loss < inst_stop_loss):\n",
    "            print(f\"Found Small Loss EARLY at start of iter: {iter+1}\")\n",
    "            break\n",
    "        # handles convergence by checking if gradient was small enough\n",
    "        if (norm_adjusted_gradient < conv_threshold and (cur_loss-past_loss) < 0):\n",
    "            print(f\"Found Convergence EARLY at start of iter: {iter + 1}\")\n",
    "            break\n",
    "        \n",
    "        # calculating approximate gradient with forward finite difference\n",
    "        for i, _ in enumerate(cur_theta):  \n",
    "            h[i] = pertebation\n",
    "            fd_gradient[i] = (loss_func(cur_theta + h) - cur_loss) / h[i]\n",
    "            h[i] = 0\n",
    "\n",
    "        # applying adam propogation\n",
    "        E_g = decay_rate1 * E_g + (1-decay_rate1) * fd_gradient\n",
    "        E_g2 = decay_rate2 * E_g2 + (1-decay_rate2) * np.square(fd_gradient)\n",
    "\n",
    "        bias_cor_E_g = E_g / (1 - decay_rate1 ** (iter+1))\n",
    "        bias_cor_E_g2 = E_g2 / (1 - decay_rate2 ** (iter+1))\n",
    "\n",
    "        adjusted_gradient = bias_cor_E_g / (np.sqrt(bias_cor_E_g2) + epsilon)\n",
    "        cur_theta = cur_theta - learning_rate * adjusted_gradient\n",
    "        \n",
    "        # saves the norm adjusted gradient  \n",
    "        norm_adjusted_gradient = np.linalg.norm(adjusted_gradient)\n",
    "        \n",
    "        if(print_message):\n",
    "            print(f\"Norm of Adam adjusted gradient: {norm_adjusted_gradient}\")\n",
    "            \n",
    "        past_loss = cur_loss\n",
    "\n",
    "\n",
    "\n",
    "    loss = loss_func(cur_theta)\n",
    "    \n",
    "    result = OptimizerResult()\n",
    "    result.fun = loss\n",
    "    result.jac = fd_gradient\n",
    "    result.x = cur_theta\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mplhep as hep\n",
    "\n",
    "hep.style.use(hep.style.CMS)\n",
    "\n",
    "def plot_hist_optimizer(optimizer_record, optimizer, topic_str, xlabel, save_location, showTitle=False):\n",
    "    plt.hist(optimizer_record)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    if (showTitle):\n",
    "        plt.title(topic_str + f\" - Alternating Ansatz + KL divergence + {optimizer}\")\n",
    "    plt.savefig(save_location, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "def plot_all_histogram(record, optimizers, topic_str, xlabel, save_location, showTitle=False):\n",
    "    flat_record = [i for list in record for i in list]   \n",
    "    bins = np.histogram(flat_record, bins=25)[1]\n",
    "\n",
    "    plt.hist(record, bins=bins, alpha = 0.7, label = optimizers)\n",
    "\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Frequecy\")\n",
    "    plt.legend()\n",
    "\n",
    "    if(showTitle):\n",
    "        plt.title(\"Main \" + topic_str + f\" - Alternating Ansatz + KL divergence\")\n",
    "\n",
    "    plt.savefig(save_location, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "def plot_strip_plot(record, optimizers, topic_str, ylabel, save_location, includeBox=True, logScale=False, showTitle=False, invertYAxis=False):\n",
    "    if (invertYAxis):\n",
    "        plt.gca().invert_yaxis()\n",
    "    \n",
    "    reformatted_record = [(element, optimizers[i]) for i, optimizer_record in enumerate(record) for element in optimizer_record]\n",
    "    df = pd.DataFrame(reformatted_record, columns=[ylabel, \"Optimizers\"])\n",
    "    \n",
    "    ax = sns.stripplot(data=df, x=\"Optimizers\", y=ylabel, hue=\"Optimizers\", legend=False)\n",
    "\n",
    "    if (includeBox):\n",
    "        sns.boxplot(data=df, x=\"Optimizers\", y=ylabel, showcaps=False, boxprops={'facecolor':'None'},\n",
    "                    showfliers=False,whiskerprops={'linewidth':0})\n",
    "    \n",
    "    if (logScale):\n",
    "        plt.yscale('log')\n",
    "\n",
    "    if(showTitle):\n",
    "        plt.title(\"Main \" + topic_str + f\" Strip Plot - Alternating Ansatz + KL divergence\")\n",
    "        \n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.savefig(save_location, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for testing all optimizers\n",
    "from qiskit_algorithms.optimizers import SPSA\n",
    "from qiskit_algorithms.optimizers import GradientDescent\n",
    "from qiskit_algorithms import utils\n",
    "\n",
    "import scipy.optimize as opt\n",
    "import time\n",
    "import warnings\n",
    "from wakepy import keep\n",
    "from pybads import BADS\n",
    "\n",
    "reruns = 25\n",
    "folder = \"saved_final_no_seed_spin_chain_state\"\n",
    "# first 0-6 do reasonably well, 7-10 have poorer performance\n",
    "optimizers = [\"Powell\", \"PYBADS\", \"Parameter_Shift\", \"Finite_Difference\", \"COBYLA\", \"SPSA\", \"SPSA_Optimized\", \"Nelder-Mead\", \"L-BFGS-B\", \"COBYQA\", \"SLSQP\"]\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "function_call_records = [[-1 for _ in range(reruns)] for _ in range(len(optimizers))]\n",
    "fidelities_record = [[-1 for _ in range(reruns)] for _ in range(len(optimizers))]\n",
    "elapsed_cpu = [[-1 for _ in range(reruns)] for _ in range(len(optimizers))]\n",
    "elapsed_wall = [[-1 for _ in range(reruns)] for _ in range(len(optimizers))]\n",
    "\n",
    "# wakepy can keep computer running program and prevents screen from going to sleep\n",
    "with keep.presenting():\n",
    "    for j, optimizer in enumerate(optimizers):\n",
    "        loss_value_records = [0] * reruns\n",
    "        \n",
    "        total_elapsed_cpu = 0\n",
    "        total_elapsed_wall = 0\n",
    "\n",
    "        for i in range(0, reruns ):\n",
    "            start_cpu = time.process_time()\n",
    "            start_wall = time.perf_counter()\n",
    "\n",
    "            global loss_values\n",
    "            global thetas\n",
    "\n",
    "            loss_values = []\n",
    "            thetas = []\n",
    "\n",
    "            theta = initialize_theta_random(circ_depth=depth, num_qbits = n_qubits)\n",
    "            theta_vector = np.reshape(theta, theta.size)\n",
    "\n",
    "            # handles seperate case of PYBADS \n",
    "            if (optimizer == \"PYBADS\"):\n",
    "                # establish bounds \n",
    "                lb = np.full(theta.size, -10.0)\n",
    "                ub = np.full(theta.size, 10.0)\n",
    "\n",
    "                plb = np.full(theta.size, -2.0*pi)\n",
    "                pub = np.full(theta.size, 2.0*pi)\n",
    "\n",
    "                # try to find the minimal\n",
    "                bads = BADS(compute_kl_loss_filled_quiet, theta_vector, lb, ub, plb, pub)\n",
    "                results = bads.optimize()\n",
    "            # handles case of SPSA\n",
    "            elif (optimizer == \"SPSA\"):\n",
    "                spsa = SPSA(maxiter=300)\n",
    "                results = spsa.minimize(compute_kl_loss_filled, theta_vector)\n",
    "\n",
    "            # handles SPSA with hyperparameters adjusted with random search optimization \n",
    "            elif (optimizer == \"SPSA_Optimized\"):\n",
    "                spsa = SPSA(maxiter=300)\n",
    "\n",
    "                # coefficients for SPSA (these are just the defaults)\n",
    "                c=0.1258\n",
    "                A=0.3186\n",
    "                a1=0.4739\n",
    "                alpha=0.6374\n",
    "                gamma=0.06059\n",
    "\n",
    "                (lr, p) = spsa.calibrate(compute_kl_loss_filled, theta_vector, c=c, stability_constant = A, target_magnitude=a1, alpha=alpha, gamma=gamma)\n",
    "                spsa.learning_rate = lr\n",
    "                spsa.perturbation = p\n",
    "                \n",
    "                results = spsa.minimize(compute_kl_loss_filled, theta_vector)\n",
    "            # handles case of finite difference gradient descent (forward approach)\n",
    "            elif (optimizer == \"Finite_Difference\"): \n",
    "                maxiter = 300\n",
    "                l_rate = 0.05\n",
    "                d_rate1 = 0.9\n",
    "                d_rate2 = 0.999\n",
    "                pert = 0.08\n",
    "                inst_stop_loss = 0.08\n",
    "                conv = 0.3\n",
    "\n",
    "                results = finite_difference_forward_adam(compute_kl_loss_filled_quiet, maxiter, l_rate, d_rate1, d_rate2, epsilon_loss, theta_vector, pertebation=pert, inst_stop_loss=inst_stop_loss, conv_threshold=conv)\n",
    "\n",
    "            elif (optimizer == \"Parameter_Shift\"):\n",
    "                maxiter = 300\n",
    "                l_rate = 0.02\n",
    "                d_rate1 = 0.9\n",
    "                d_rate2 = 0.999\n",
    "                conv = 0.2\n",
    "                loss = 0.08\n",
    "\n",
    "                results = param_shift_gradient_descent_adam(maxiter, l_rate, d_rate1, d_rate2, epsilon_loss, theta_vector, basis_list, GHZ_measurement, shots, inst_stop_loss=loss, conv_threshold=conv)\n",
    "            else:\n",
    "                # establish bounds\n",
    "                bounds = [(-2.0*pi, 2.0*pi) for i in range(0, theta.size)]\n",
    "                args = (ansatz_isa_list, basis_list, GHZ_measurement, shots, param_vec)\n",
    "\n",
    "                # try to find the minimal\n",
    "                results = opt.minimize(compute_kl_loss, theta_vector, args = args, method = optimizer, bounds=bounds, options={\"maxiter\":max_iter})\n",
    "            \n",
    "            \n",
    "            loss_value_records[i] = loss_values\n",
    "\n",
    "            end_cpu = time.process_time()\n",
    "            end_wall = time.perf_counter()\n",
    "\n",
    "            elapsed_cpu[j][i] = end_cpu - start_cpu\n",
    "            elapsed_wall[j][i] = end_wall - start_wall\n",
    "            total_elapsed_cpu += elapsed_cpu[j][i]\n",
    "            total_elapsed_wall += elapsed_wall[j][i]\n",
    "\n",
    "            ansatz_state = Statevector.from_instruction(ansatz.assign_parameters({param_vec : results.x}))\n",
    "            fidelities_record[j][i] = qiskit.quantum_info.state_fidelity(ghz_state, ansatz_state)\n",
    "\n",
    "            if (optimizer[:15] != \"Parameter_Shift\"):\n",
    "                function_call_records[j][i] = len(loss_value_records[i])\n",
    "            else:\n",
    "                # parameter shift function calls is a better analogue to the number loss_values found in other optimizers\n",
    "                # since parameter shift only calls the loss function once per iteration\n",
    "                # However, the parameter shift itself runs the circuit some times as well. \n",
    "                function_call_records[j][i] = results.nfev\n",
    "                np.save(f\"{folder}/{optimizer}/num_function_calls_iter{i}.npy\", function_call_records[j][i])\n",
    "\n",
    "            # save all of the results as npy\n",
    "            np.save(f\"{folder}/{optimizer}/params_training_iter{i}.npy\", thetas)\n",
    "            np.save(f\"{folder}/{optimizer}/loss_training_iter{i}.npy\", loss_values)\n",
    "            np.save(f\"{folder}/{optimizer}/final_theta_iter{i}.npy\", results.x)\n",
    "            np.save(f\"{folder}/{optimizer}/fidelity_iter{i}.npy\", fidelities_record[j][i])\n",
    "            np.save(f\"{folder}/{optimizer}/elapsed_cpu_times_iter{i}.npy\", elapsed_cpu[j][i])\n",
    "            np.save(f\"{folder}/{optimizer}/elapsed_wall_times_iter{i}.npy\", elapsed_wall[j][i])\n",
    "\n",
    "\n",
    "        # makes a summary result txt file for readability\n",
    "        with open(f\"{folder}/all_optimizers_summary.txt\", 'a') as file:\n",
    "            file.write(f\"[OPTIMIZER]: {optimizer}\\n\")\n",
    "\n",
    "            file.write(f\"AVG Wall time elapsed: {total_elapsed_wall/reruns}\\n\")\n",
    "            file.write(f\"ALL Wall times elapsed: {elapsed_wall[j]}\\n\")\n",
    "            file.write(f\"AVG CPU time elapsed: {total_elapsed_cpu/reruns}\\n\")\n",
    "            file.write(f\"ALL CPU times elapsed: {elapsed_cpu[j]}\\n\")\n",
    "            \n",
    "            file.write(f\"Function Calls: {function_call_records[j]}\\n\")\n",
    "\n",
    "            file.write(f\"Fidelities: {fidelities_record[j]}\\n\\n\")\n",
    "\n",
    "        # plot the loss function and save the plots\n",
    "        for i in range(0, reruns):\n",
    "            plt.plot(loss_value_records[i], label=f\"KL Loss \" + str(i), alpha=0.5)\n",
    "\n",
    "        # plot the loss function over the total number of loss function calls and save plots\n",
    "        plt.ylim(0, 3)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Loss - Alternating Ansatz + KL divergence + {optimizer}\")\n",
    "        plt.savefig(f\"{folder}/{optimizer}/plots_loss_func.png\", bbox_inches='tight')\n",
    "        plt.clf()\n",
    "\n",
    "        # plot the number of function calls in a histogram\n",
    "        plot_hist_optimizer(function_call_records[j], optimizer, \"Func Call\", \"Function calls\", f\"{folder}/{optimizer}/plots_func_call.png\")\n",
    "\n",
    "        # plot the wall times in a histogram\n",
    "        plot_hist_optimizer(elapsed_wall[j], optimizer, \"Wall Time\", \"Wall Time (sec)\", f\"{folder}/{optimizer}/plots_wall_time.png\")\n",
    "\n",
    "        # plot the cpu times in a histogram\n",
    "        plot_hist_optimizer(elapsed_cpu[j], optimizer, \"CPU Time\", \"CPU Time (sec)\", f\"{folder}/{optimizer}/plots_cpu_time.png\")\n",
    "\n",
    "        # plot the fidelity as a histogram with horizontal axis being fidelity and verticle axis is frequency and save plots.\n",
    "        plot_hist_optimizer(fidelities_record[j], optimizer, \"Fidelity Histogram\", \"Fidelity\", f\"{folder}/{optimizer}/plots_fidelity_hist.png\")\n",
    "\n",
    "        # plot the fidelity as a bar graph (verticle axis is fidelity and horizontal is trial number) and save plots\n",
    "        plt.bar([f\"Trial {i}\" for i in range(0, reruns)], fidelities_record[j])\n",
    "        plt.xticks(rotation = 45)\n",
    "        plt.ylabel(\"Fidelity\")\n",
    "        plt.title(f\"Fidelity Bar Graph - Alternating Ansatz + KL divergence + {optimizer}\")\n",
    "\n",
    "        plt.savefig(f\"{folder}/{optimizer}/plots_fidelity_bar.png\", bbox_inches='tight')\n",
    "        plt.clf()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
